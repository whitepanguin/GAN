# -----------------------------
# ğŸŒ» [1] ê³µí†µ ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ì •ì˜
# -----------------------------
import os, math, json, random
import torch
import numpy as np
from pathlib import Path
from tqdm.auto import tqdm
from torchvision import transforms
from torch.utils.data import DataLoader
from PIL import Image
from torchvision import utils
from dataclasses import dataclass, asdict
from copy import deepcopy
from typing import List, Literal, Optional

# í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì´ ì €ì¥ë  ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì„¤ì • (Colabì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥)
PROJECT_DIR = Path("./dept50")

# í•™ìŠµ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” ì´ë¯¸ì§€ í´ë” ë£¨íŠ¸ (ì˜ˆ: /content/sunflower_all/)
DATA_ROOT = Path("./bellflower/classA/")

# ê²°ê³¼ ì €ì¥ìš© í´ë” ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
RUNS_DIR = PROJECT_DIR / "runsdept5"
RUNS_DIR.mkdir(parents=True, exist_ok=True)

# ëœë¤ ì‹œë“œ ê³ ì •: ì¬í˜„ì„± í™•ë³´ (í•­ìƒ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë„ë¡)
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# í•™ìŠµ ì¥ì¹˜ ì„¤ì •: GPUê°€ ê°€ëŠ¥í•˜ë©´ CUDA, ì•„ë‹ˆë©´ CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] Using device: {device}")

# ë§Œì•½ CUDA ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ë©´ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì„¤ì • (ConvNetì— ì í•©)
if device.type == "cuda":
    torch.backends.cudnn.benchmark = True

# -----------------------------
# ğŸŒ» [2] Sunflower ì´ë¯¸ì§€ Dataset ì •ì˜
# -----------------------------

# ì‚¬ìš©í•  ì´ë¯¸ì§€ í™•ì¥ì ëª©ë¡ ì •ì˜ (ëª¨ë“  ì¼ë°˜ ì´ë¯¸ì§€ í¬ë§· í¬í•¨)
IMG_EXTS = (".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tif", ".tiff")

# ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def collect_image_paths(root: Path) -> List[Path]:
    # root í•˜ìœ„ í´ë”ê¹Œì§€ í¬í•¨í•˜ì—¬ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    out = []
    for ext in IMG_EXTS:
        out += list(root.rglob(f"*{ext}"))
    return sorted(out)

# PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜
class FlatImageDataset(torch.utils.data.Dataset):
    """
    ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ê³ ì •ëœ í•´ìƒë„ë¡œ Resize + Crop + ì •ê·œí™”í•˜ì—¬ ë°˜í™˜í•˜ëŠ” Dataset
    """
    def __init__(self, root_dir: Path, img_size: int):
        # ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
        self.paths = collect_image_paths(root_dir)
        if not self.paths:
            raise FileNotFoundError(f"No images found in {root_dir}")

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ Transform ì •ì˜: Resize â†’ CenterCrop â†’ Tensor â†’ ì •ê·œí™”
        self.tfm = transforms.Compose([
            transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(img_size),
            transforms.ToTensor(),  # (C, H, W) í˜•íƒœë¡œ ë³€í™˜ë¨
            transforms.Normalize([0.5]*3, [0.5]*3)  # í”½ì…€ê°’ [-1, 1]ë¡œ ì •ê·œí™”
        ])

    def __len__(self):
        # ì „ì²´ ì´ë¯¸ì§€ ê°œìˆ˜ ë°˜í™˜
        return len(self.paths)

    def __getitem__(self, i):
        # ië²ˆì§¸ ì´ë¯¸ì§€ ë¡œë”© â†’ RGB ë³€í™˜ â†’ ì „ì²˜ë¦¬ í›„ ë°˜í™˜
        return self.tfm(Image.open(self.paths[i]).convert("RGB"))
# -----------------------------
# ğŸŒ» [3] DataLoader ìƒì„± í•¨ìˆ˜
# -----------------------------

def make_loader(ds, bs, train=True):
    """
    ì£¼ì–´ì§„ Datasetì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¡œë”©í•  DataLoader ìƒì„±
    - num_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (Colabì´ë©´ 2~4 ê¶Œì¥)
    - pin_memory: CUDA ì„±ëŠ¥ í–¥ìƒ ì˜µì…˜
    - drop_last: ë°°ì¹˜ê°€ ëª¨ìë€ ê²½ìš° ë²„ë¦¬ê¸°
    """
    return DataLoader(
        ds, bs,
        shuffle=train,
        num_workers=0,
        pin_memory=(device.type == 'cuda'),
        drop_last=True
    )
# -----------------------------
# ğŸŒ» [4-1] Generator ì •ì˜
# -----------------------------

import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, z_dim=128, ch=64):
        super().__init__()

        # GëŠ” ConvTranspose2d ê³„ì¸µì„ ë°˜ë³µí•˜ì—¬ 4x4 â†’ 8 â†’ 16 â†’ 32 â†’ 64 â†’ 128 ë¡œ ì—…ìƒ˜í”Œë§
        self.net = nn.Sequential(
            # ì…ë ¥ latent vector(z)ë¥¼ (ch*8)x4x4 í˜•íƒœë¡œ íˆ¬ì˜
            nn.ConvTranspose2d(z_dim, ch * 8, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(ch * 8),
            nn.ReLU(True),  # (ch*8)x4x4

            nn.ConvTranspose2d(ch * 8, ch * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 4),
            nn.ReLU(True),  # (ch*4)x8x8

            nn.ConvTranspose2d(ch * 4, ch * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 2),
            nn.ReLU(True),  # (ch*2)x16x16

            nn.ConvTranspose2d(ch * 2, ch, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch),
            nn.ReLU(True),  # chx32x32

            nn.ConvTranspose2d(ch, ch // 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch // 2),
            nn.ReLU(True),  # (ch//2)x64x64

            nn.ConvTranspose2d(ch // 2, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()  # ì¶œë ¥ ì´ë¯¸ì§€: 3ì±„ë„ RGB, í¬ê¸° 128x128, í”½ì…€ê°’ ë²”ìœ„ [-1, 1]
        )

    def forward(self, z):
        # ì…ë ¥ zëŠ” (batch_size, z_dim) â†’ reshape to (batch_size, z_dim, 1, 1)
        return self.net(z.view(z.size(0), z.size(1), 1, 1))
# -----------------------------
# ğŸŒ» [4-2] Discriminator ì •ì˜
# -----------------------------

class Discriminator(nn.Module):
    def __init__(self, ch=64):
        super().__init__()

        # DëŠ” Conv2d ê³„ì¸µì„ ë°˜ë³µí•˜ì—¬ 128x128 ì´ë¯¸ì§€ë¥¼ 1x1 ë¡œì§“ìœ¼ë¡œ ì¤„ì„
        self.net = nn.Sequential(
            # ì…ë ¥ ì´ë¯¸ì§€ (3, 128, 128) â†’ (ch, 64, 64)
            nn.Conv2d(3, ch, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch, ch * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 2),
            nn.LeakyReLU(0.2, inplace=True),  # (ch*2)x32x32

            nn.Conv2d(ch * 2, ch * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 4),
            nn.LeakyReLU(0.2, inplace=True),  # (ch*4)x16x16

            nn.Conv2d(ch * 4, ch * 8, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 8),
            nn.LeakyReLU(0.2, inplace=True),  # (ch*8)x8x8

            nn.Conv2d(ch * 8, ch * 16, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 16),
            nn.LeakyReLU(0.2, inplace=True),  # (ch*16)x4x4

            nn.Conv2d(ch * 16, 1, kernel_size=4, stride=1, padding=0, bias=False),
            # ì¶œë ¥: (batch, 1, 1, 1) â†’ flattení•˜ë©´ ë¡œì§“ 1ê°œë¡œ ë³€í™˜ë¨
        )

    def forward(self, x):
        return self.net(x).view(-1)  # ë¡œì§“ í•˜ë‚˜ë¡œ flatten
# -----------------------------
# ğŸŒ» [5] DCGAN-style ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
# -----------------------------

def weights_init(m):
    """
    DCGAN ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ì´ˆê¸°í™” ë°©ì‹:
    - Conv, ConvTranspose: í‰ê·  0, í‘œì¤€í¸ì°¨ 0.02 ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”
    - BatchNorm: í‰ê·  1, í‘œì¤€í¸ì°¨ 0.02 (Î³), í¸í–¥ì€ 0 (Î²)
    """
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
# -----------------------------
# ğŸŒ» [6] ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
# -----------------------------

# ìƒì„±ì ì†ì‹¤ í•¨ìˆ˜ (G)
def generator_loss(fake_logit):
    """
    Gì˜ ëª©í‘œëŠ” Dë¥¼ ì†ì´ëŠ” ê²ƒ (ì¦‰, D(fake)ë¥¼ 'real'ë¡œ ë¶„ë¥˜í•˜ê²Œ ë§Œë“¤ê¸°)
    â†’ BCE ê¸°ì¤€, targetì€ 1 (ì§„ì§œì²˜ëŸ¼ ë³´ì´ë„ë¡)
    """
    target = torch.ones_like(fake_logit)  # ëª¨ë“  ê°’ì„ 1ë¡œ ì„¤ì •
    return nn.BCEWithLogitsLoss()(fake_logit, target)

# íŒë³„ì ì†ì‹¤ í•¨ìˆ˜ (D)
def discriminator_loss(real_logit, fake_logit):
    """
    Dì˜ ëª©í‘œëŠ” ì§„ì§œ(real)ëŠ” 1, ê°€ì§œ(fake)ëŠ” 0ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒ
    - real: target=1
    - fake: target=0
    """
    real_target = torch.ones_like(real_logit)
    fake_target = torch.zeros_like(fake_logit)
    real_loss = nn.BCEWithLogitsLoss()(real_logit, real_target)
    fake_loss = nn.BCEWithLogitsLoss()(fake_logit, fake_target)
    return real_loss + fake_loss

# -----------------------------
# ğŸŒ» [7] í•œ ì¡°í•©ì— ëŒ€í•´ ì „ì²´ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
# -----------------------------

def train_one(cfg, dataset):
    # ğŸ”¸ ì‹¤í–‰ ê²½ë¡œ ì¤€ë¹„
    RUN_DIR = RUNS_DIR / cfg.name
    SAMPLES_DIR = RUN_DIR / "samples"
    CKPT_DIR = RUN_DIR / "checkpoints"
    for d in [RUN_DIR, SAMPLES_DIR, CKPT_DIR]:
        d.mkdir(parents=True, exist_ok=True)

    # ğŸ”¸ ì„¤ì • ì €ì¥ (ë‚˜ì¤‘ì— ë¶„ì„ìš©)
    with open(RUN_DIR / "config.json", "w") as f:
        json.dump(asdict(cfg), f, indent=2)

    # ğŸ”¸ ë°ì´í„°ë¡œë” ìƒì„±
    dl = make_loader(dataset, cfg.batch_size)

    # ğŸ”¸ ëª¨ë¸ ìƒì„± ë° ì´ˆê¸°í™”
    G = Generator(cfg.z_dim, cfg.g_ch).to(device)
    D = Discriminator(cfg.d_ch).to(device)
    G.apply(weights_init)
    D.apply(weights_init)

    # ğŸ”¸ Optimizer ì„¤ì • (Adam)
    opt_G = torch.optim.Adam(G.parameters(), lr=cfg.lr_g, betas=cfg.betas)
    opt_D = torch.optim.Adam(D.parameters(), lr=cfg.lr_d, betas=cfg.betas)

    # ğŸ”¸ ê³ ì • latent ë²¡í„° (ìƒ˜í”Œìš©)
    fixed_z = torch.randn(cfg.n_samples, cfg.z_dim, device=device)

    # ğŸ”¸ í•™ìŠµ ë£¨í”„ ì‹œì‘
    for epoch in range(1, cfg.epochs + 1):
        G.train(); D.train()
        loss_G_epoch = 0.0
        loss_D_epoch = 0.0

        pbar = tqdm(dl, desc=f"[{cfg.name}] Epoch {epoch}/{cfg.epochs}", leave=False)

        for real in pbar:
            real = real.to(device)

            B = real.size(0)  # í˜„ì¬ ë°°ì¹˜ í¬ê¸°

            # ------------------------
            # â‘  D í•™ìŠµ
            # ------------------------
            opt_D.zero_grad()

            # latent â†’ fake ì´ë¯¸ì§€ ìƒì„±
            z = torch.randn(B, cfg.z_dim, device=device)
            fake = G(z)

            # Dì— í†µê³¼ì‹œì¼œ íŒë³„ê°’(logit) ì–»ê¸°
            real_logit = D(real)
            fake_logit = D(fake.detach())

            # ì†ì‹¤ ê³„ì‚° + ì—­ì „íŒŒ
            loss_D = discriminator_loss(real_logit, fake_logit)
            loss_D.backward()
            opt_D.step()

            # ------------------------
            # â‘¡ G í•™ìŠµ
            # ------------------------
            opt_G.zero_grad()

            # ë‹¤ì‹œ G â†’ D í†µê³¼
            gen = G(z)
            gen_logit = D(gen)

            # ì†ì‹¤ ê³„ì‚° + ì—­ì „íŒŒ
            loss_G = generator_loss(gen_logit)
            loss_G.backward()
            opt_G.step()

            # ì†ì‹¤ê°’ ëˆ„ì 
            loss_G_epoch += loss_G.item() * B
            loss_D_epoch += loss_D.item() * B

        # ğŸ”¸ í‰ê·  ì†ì‹¤ ì¶œë ¥
        loss_G_epoch /= len(dataset)
        loss_D_epoch /= len(dataset)
        print(f"[{cfg.name}] Epoch {epoch:4d} | loss_G={loss_G_epoch:.4f} | loss_D={loss_D_epoch:.4f}")

        # ğŸ”¸ 50 epochë§ˆë‹¤: 1ë°°ì¹˜ ìƒì„± ì´ë¯¸ì§€ ì €ì¥
        if epoch % cfg.sample_batch_every == 0:
            G.eval()
            with torch.no_grad():
                sample_dir = SAMPLES_DIR / f"ep_{epoch:04d}"
                sample_dir.mkdir(parents=True, exist_ok=True)

                z = torch.randn(cfg.batch_size, cfg.z_dim, device=device)
                fake = G(z).cpu()

                # ê°œë³„ ì´ë¯¸ì§€ ì €ì¥
                for i in range(cfg.batch_size):
                    utils.save_image(
                        fake[i], sample_dir / f"fake_{i:04d}.png",
                        normalize=True, value_range=(-1, 1)
                    )

                # ê·¸ë¦¬ë“œ ì €ì¥ (nrow: ê°€ë¡œ í–‰ ìˆ˜)
                utils.save_image(
                    fake, sample_dir / f"grid.png",
                    nrow=int(math.sqrt(cfg.batch_size)),
                    normalize=True, value_range=(-1, 1)
                )

        # ğŸ”¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (100 epochë§ˆë‹¤)
        if epoch % 100 == 0:
            ckpt = {
                "G": G.state_dict(),
                "D": D.state_dict(),
                "opt_G": opt_G.state_dict(),
                "opt_D": opt_D.state_dict(),
                "epoch": epoch,
                "cfg": asdict(cfg)
            }
            torch.save(ckpt, CKPT_DIR / f"e{epoch:04d}.pt")
# -----------------------------
# ğŸŒ» [8] 5ê°€ì§€ ì‹¤í—˜ ì¡°í•© ì •ì˜
# -----------------------------

@dataclass
class RunConfig:
    name: str
    img_size: int = 128
    batch_size: int = 64
    z_dim: int = 128
    g_ch: int = 64
    d_ch: int = 64
    lr_g: float = 2e-4
    lr_d: float = 2e-4
    betas: tuple = (0.5, 0.999)
    epochs: int = 500
    sample_batch_every: int = 50
    n_samples: int = 64

RUNS: List[RunConfig] = [
    RunConfig(name="exp1_default"),
    RunConfig(name="exp2_bigG", g_ch=96, z_dim=192),
    RunConfig(name="exp3_bigD", d_ch=128),
    RunConfig(name="exp4_lr_decay", lr_d=1e-4),
    RunConfig(name="exp5_deep", g_ch=128, d_ch=128, z_dim=256),
]
# -----------------------------
# ğŸŒ» [9] ì „ì²´ ì‹¤í–‰
# -----------------------------
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image


if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ì¤€ë¹„
    dataset = FlatImageDataset(DATA_ROOT, img_size=128)

    # ëª¨ë“  ì¡°í•© ìˆœì°¨ ì‹¤í–‰
    for cfg in RUNS:
        try:
            train_one(cfg, dataset)
        except Exception as e:
            print(f"[ERROR] Run failed: {cfg.name} => {repr(e)}")

