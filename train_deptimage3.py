# -*- coding: utf-8 -*-
"""
ì†Œê·œëª¨(873ì¥) ë°ì´í„°ì…‹ì—ì„œ DCGAN í•™ìŠµ ì•ˆì •í™” + ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ

ì ìš©í•œ ë³€ê²½ì 
- Data Augmentation ê°•í™” (flip/rotation/color jitter)
- Label Smoothing (Dì˜ real=0.9, fake=0.1)
- í•™ìŠµë¥  ë¹„ëŒ€ì¹­: lr_g=2e-4, lr_d=1e-4
- ì²´í¬í¬ì¸íŠ¸ ìë™ ì¬ê°œ: ./dept50/runsdept5/<run_name>/checkpoints/eXXXX.pt ì¤‘ ê°€ì¥ ìµœì‹ ë¶€í„° ì´ì–´ì„œ í•™ìŠµ

â€» ê¸°ì¡´ì— í•™ìŠµí•˜ë˜ ì‹¤í—˜(exp1_default ë“±)ì˜ ì•„í‚¤í…ì²˜(G/D ì±„ë„ ìˆ˜)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ ì¬ê°œê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
  ì•„ë˜ RUNS ë¦¬ìŠ¤íŠ¸ëŠ” ì´ì „ê³¼ ë™ì¼í•œ ì•„í‚¤í…ì²˜ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import os, math, json, random, re
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import List, Literal, Optional

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms, utils
from PIL import Image
from tqdm.auto import tqdm

# -----------------------------
# ğŸŒ» [1] ê³µí†µ ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ì •ì˜
# -----------------------------
PROJECT_DIR = Path("./dept50")
DATA_ROOT = Path("./bellflower/classA/")
RUNS_DIR = PROJECT_DIR / "runsdept5"
RUNS_DIR.mkdir(parents=True, exist_ok=True)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] Using device: {device}")
if device.type == "cuda":
    torch.backends.cudnn.benchmark = True

# -----------------------------
# ğŸŒ» [2] Dataset ì •ì˜ (ê°•í™”ëœ Augmentation)
# -----------------------------
IMG_EXTS = (".jpg", ".jpeg", ".png", ".webp", ".bmp", ".tif", ".tiff")

def collect_image_paths(root: Path) -> List[Path]:
    out = []
    for ext in IMG_EXTS:
        out += list(root.rglob(f"*{ext}"))
    return sorted(out)

class FlatImageDataset(torch.utils.data.Dataset):
    """
    ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ê³ ì • í•´ìƒë„ë¡œ ë³€í™˜. ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì•ˆì •í™”ë¥¼ ìœ„í•´ Augmentation ê°•í™”.
    """
    def __init__(self, root_dir: Path, img_size: int):
        self.paths = collect_image_paths(root_dir)
        if not self.paths:
            raise FileNotFoundError(f"No images found in {root_dir}")

        self.tfm = transforms.Compose([
            transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(img_size),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize([0.5]*3, [0.5]*3)
        ])

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, i):
        return self.tfm(Image.open(self.paths[i]).convert("RGB"))

# -----------------------------
# ğŸŒ» [3] DataLoader
# -----------------------------

def make_loader(ds, bs, train=True):
    return DataLoader(
        ds, bs,
        shuffle=train,
        num_workers=0,
        pin_memory=(device.type == 'cuda'),
        drop_last=True
    )

# -----------------------------
# ğŸŒ» [4-1] Generator
# -----------------------------
class Generator(nn.Module):
    def __init__(self, z_dim=128, ch=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.ConvTranspose2d(z_dim, ch * 8, kernel_size=4, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(ch * 8),
            nn.ReLU(True),

            nn.ConvTranspose2d(ch * 8, ch * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 4),
            nn.ReLU(True),

            nn.ConvTranspose2d(ch * 4, ch * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(ch * 2, ch, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch),
            nn.ReLU(True),

            nn.ConvTranspose2d(ch, ch // 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch // 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(ch // 2, 3, kernel_size=4, stride=2, padding=1, bias=False),
            nn.Tanh()
        )
    def forward(self, z):
        return self.net(z.view(z.size(0), z.size(1), 1, 1))

# -----------------------------
# ğŸŒ» [4-2] Discriminator (ì•„í‚¤í…ì²˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼ ìœ ì§€ â€” ì¬ê°œ í˜¸í™˜ì„±)
# -----------------------------
class Discriminator(nn.Module):
    def __init__(self, ch=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(3, ch, kernel_size=4, stride=2, padding=1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch, ch * 2, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 2),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch * 2, ch * 4, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 4),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch * 4, ch * 8, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 8),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch * 8, ch * 16, kernel_size=4, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(ch * 16),
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(ch * 16, 1, kernel_size=4, stride=1, padding=0, bias=False),
        )
    def forward(self, x):
        return self.net(x).view(-1)

# -----------------------------
# ğŸŒ» [5] DCGAN-style ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
# -----------------------------

def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# -----------------------------
# ğŸŒ» [6] ì†ì‹¤ í•¨ìˆ˜ (Label Smoothing ì ìš©)
# -----------------------------

def generator_loss(fake_logit):
    target = torch.ones_like(fake_logit)  # GëŠ” Dë¥¼ ì†ì´ë„ë¡ 1ì— ê°€ê¹ê²Œ
    return nn.BCEWithLogitsLoss()(fake_logit, target)

# Label smoothing: real=0.9, fake=0.1
REAL_SMOOTH = 0.9
FAKE_SMOOTH = 0.1

def discriminator_loss(real_logit, fake_logit):
    real_target = torch.full_like(real_logit, REAL_SMOOTH)
    fake_target = torch.full_like(fake_logit, FAKE_SMOOTH)
    real_loss = nn.BCEWithLogitsLoss()(real_logit, real_target)
    fake_loss = nn.BCEWithLogitsLoss()(fake_logit, fake_target)
    return real_loss + fake_loss

# -----------------------------
# ğŸŒ» [7] ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸ (ê°€ì¥ ìµœì‹  ckpt ë¶ˆëŸ¬ì˜¤ê¸°)
# -----------------------------

def find_latest_ckpt(ckpt_dir: Path) -> Optional[Path]:
    if not ckpt_dir.exists():
        return None
    cands = sorted(ckpt_dir.glob("e*.pt"))
    if not cands:
        return None
    # íŒŒì¼ëª… eXXXX.ptì˜ ìˆ«ìë¡œ ì •ë ¬
    def epnum(p: Path):
        m = re.search(r"e(\d+)\.pt", p.name)
        return int(m.group(1)) if m else -1
    cands.sort(key=epnum)
    return cands[-1]

# -----------------------------
# ğŸŒ» [8] íŠ¸ë ˆì´ë‹ ë£¨í”„ (ì´ì–´í•™ìŠµ ì§€ì›)
# -----------------------------

def train_one(cfg, dataset):
    RUN_DIR = RUNS_DIR / cfg.name
    SAMPLES_DIR = RUN_DIR / "samples"
    CKPT_DIR = RUN_DIR / "checkpoints"
    for d in [RUN_DIR, SAMPLES_DIR, CKPT_DIR]:
        d.mkdir(parents=True, exist_ok=True)

    with open(RUN_DIR / "config.json", "w") as f:
        json.dump(asdict(cfg), f, indent=2)

    dl = make_loader(dataset, cfg.batch_size)

    G = Generator(cfg.z_dim, cfg.g_ch).to(device)
    D = Discriminator(cfg.d_ch).to(device)

    start_epoch = 1
    total_epochs = cfg.epochs + getattr(cfg, 'extra_epochs', 0)

    opt_G = torch.optim.Adam(G.parameters(), lr=cfg.lr_g, betas=cfg.betas)
    opt_D = torch.optim.Adam(D.parameters(), lr=cfg.lr_d, betas=cfg.betas)

    if cfg.resume:
        latest = find_latest_ckpt(CKPT_DIR)
        if latest is not None:
            print(f"[INFO] Resuming from {latest}")
            ckpt = torch.load(latest, map_location=device)
            G.load_state_dict(ckpt["G"])  # ì•„í‚¤í…ì²˜ ë™ì¼ ì „ì œ
            D.load_state_dict(ckpt["D"])
            if "opt_G" in ckpt and "opt_D" in ckpt:
                try:
                    opt_G.load_state_dict(ckpt["opt_G"])
                    opt_D.load_state_dict(ckpt["opt_D"])
                except Exception as e:
                    print(f"[WARN] Optimizer state load failed: {e}. Recreating optimizers.")
                    opt_G = torch.optim.Adam(G.parameters(), lr=cfg.lr_g, betas=cfg.betas)
                    opt_D = torch.optim.Adam(D.parameters(), lr=cfg.lr_d, betas=cfg.betas)
            start_epoch = int(ckpt.get("epoch", 0)) + 1
            print(f"[INFO] Resume start epoch: {start_epoch}")
        else:
            print("[INFO] No checkpoint found. Training from scratch.")
            G.apply(weights_init)
            D.apply(weights_init)
    else:
        G.apply(weights_init)
        D.apply(weights_init)

    fixed_z = torch.randn(cfg.n_samples, cfg.z_dim, device=device)

    for epoch in range(start_epoch, total_epochs + 1):
        G.train(); D.train()
        loss_G_epoch = 0.0
        loss_D_epoch = 0.0

        pbar = tqdm(dl, desc=f"[{cfg.name}] Epoch {epoch}/{total_epochs}", leave=False)

        for real in pbar:
            real = real.to(device)
            B = real.size(0)

            # â‘  D í•™ìŠµ
            opt_D.zero_grad()
            z = torch.randn(B, cfg.z_dim, device=device)
            fake = G(z)
            real_logit = D(real)
            fake_logit = D(fake.detach())
            loss_D = discriminator_loss(real_logit, fake_logit)
            loss_D.backward()
            opt_D.step()

            # â‘¡ G í•™ìŠµ
            opt_G.zero_grad()
            gen = G(z)  # ê°™ì€ z ë‹¤ì‹œ ì‚¬ìš©í•´ë„ ë¬´ë°©
            gen_logit = D(gen)
            loss_G = generator_loss(gen_logit)
            loss_G.backward()
            opt_G.step()

            loss_G_epoch += loss_G.item() * B
            loss_D_epoch += loss_D.item() * B

        loss_G_epoch /= len(dataset)
        loss_D_epoch /= len(dataset)
        print(f"[{cfg.name}] Epoch {epoch:4d} | loss_G={loss_G_epoch:.4f} | loss_D={loss_D_epoch:.4f}")

        # ìƒ˜í”Œ ì €ì¥
        if epoch % cfg.sample_batch_every == 0:
            G.eval()
            with torch.no_grad():
                sample_dir = SAMPLES_DIR / f"ep_{epoch:04d}"
                sample_dir.mkdir(parents=True, exist_ok=True)
                z = torch.randn(cfg.batch_size, cfg.z_dim, device=device)
                fake = G(z).cpu()
                for i in range(cfg.batch_size):
                    utils.save_image(
                        fake[i], sample_dir / f"fake_{i:04d}.png",
                        normalize=True, value_range=(-1, 1)
                    )
                utils.save_image(
                    fake, sample_dir / f"grid.png",
                    nrow=int(math.sqrt(cfg.batch_size)),
                    normalize=True, value_range=(-1, 1)
                )

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (100 epochë§ˆë‹¤)
        if epoch % 100 == 0:
            ckpt = {
                "G": G.state_dict(),
                "D": D.state_dict(),
                "opt_G": opt_G.state_dict(),
                "opt_D": opt_D.state_dict(),
                "epoch": epoch,
                "cfg": asdict(cfg)
            }
            torch.save(ckpt, CKPT_DIR / f"e{epoch:04d}.pt")

# -----------------------------
# ğŸŒ» [9] ì‹¤í—˜ ì„¤ì • (ì•„í‚¤í…ì²˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
# -----------------------------
@dataclass
class RunConfig:
    name: str
    img_size: int = 128
    batch_size: int = 64
    z_dim: int = 128
    g_ch: int = 64
    d_ch: int = 64
    lr_g: float = 2e-4
    lr_d: float = 1e-4  # Dë¥¼ ë” ë‚®ê²Œ
    betas: tuple = (0.5, 0.999)
    epochs: int = 500   # ê¸°ë³¸ ëª©í‘œ ì—í­(ì´ì „ì— 500ìœ¼ë¡œ ëŒë¦¬ì…¨ìŒ)
    extra_epochs: int = 0  # ì´ì–´í•™ìŠµ ì‹œ ì¶”ê°€ë¡œ ë” ëŒë¦¬ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ëŠ˜ë¦¬ì„¸ìš”
    sample_batch_every: int = 50
    n_samples: int = 64
    resume: bool = True

RUNS: List[RunConfig] = [
    RunConfig(name="exp1_default"),
    # RunConfig(name="exp2_bigG", g_ch=96, z_dim=192),
    # RunConfig(name="exp3_bigD", d_ch=128),
    # RunConfig(name="exp4_lr_decay", lr_d=1e-4),
    RunConfig(name="exp5_deep", g_ch=128, d_ch=128, z_dim=256),
]

# -----------------------------
# ğŸŒ» [10] ì „ì²´ ì‹¤í–‰
# -----------------------------
if __name__ == "__main__":
    dataset = FlatImageDataset(DATA_ROOT, img_size=128)
    for cfg in RUNS:
        try:
            train_one(cfg, dataset)
        except Exception as e:
            print(f"[ERROR] Run failed: {cfg.name} => {repr(e)}")
